{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82163b8b",
   "metadata": {},
   "source": [
    "# Customizing open-source AWS XGBoost algorithm container\n",
    "\n",
    "This notebooks demonstrates an example of customizing AWS XGBoost algorithm container. We are basing our modifications on AWS builtin XGBoost algorithm version 1.2-2. \n",
    "\n",
    "A docker script gets a copy of this open-source container, swaps some of the python scripts, and creates an image hosted in a private AWS ECR registry. The training and inference APIs using this customized container remain the same as the original builtin container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0be0b1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " \n",
    "# Case study: Producing SHAP values in batch inference\n",
    "\n",
    "We use the example of producing [SHAP values](https://arxiv.org/abs/1705.07874) together predications at inference time. \n",
    "SHAP values provide information on which features are contributing the most to the predicted value. SHAP values are often \n",
    "used in explaining a model's behavior and explainability.\n",
    "The [shap package](https://github.com/slundberg/shap) can be used to compute the SHAP values and comes pre-installed in the SageMaker XGBoost container. In \n",
    "training mode, SageMaker Debugger can be configured to collect SHAP values as post-training debugging logs. AWS SageMaker\n",
    "Clarify can also similarly be used to collect SHAP values in post-processing.\n",
    "Here, we are interested in computing SHAP values at inference time (online). Getting SHAP values together with prediction\n",
    "is useful in many practical situations. For example, in user churn prediction, real-time SHAP values can be used to \n",
    "identify the key drivers behind a user's likelihood to depart and personalize website's content to improve his or her \n",
    "experience.\n",
    "\n",
    "This workflow can be the baseline to tweak the image as needed. See the accompanying code for an example where two scripts\n",
    "in the official container are modified so the algorithm returns SHAP feature-importance values together with the inference prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b1523",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.session import s3_input, Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "s3_client = boto3.client(\"s3\")\n",
    "account=boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# S3 bucket where the training data is located.\n",
    "data_bucket = f\"sagemaker-sample-files\"\n",
    "data_prefix = \"datasets/tabular/uci_abalone\"\n",
    "data_bucket_path = f\"s3://{data_bucket}\"\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Can specify a different bucket and prefix\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "output_prefix = \"sagemaker/DEMO-xgboost-abalone-default\"\n",
    "output_bucket_path = f\"s3://{output_bucket}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7ec40",
   "metadata": {},
   "source": [
    "## Preparing sample data\n",
    "We use the [Abalone](https://archive.ics.uci.edu/ml/datasets/abalonehttps://archive.ics.uci.edu/ml/datasets/abalone) dataset. The sample data is already prepared in libsvm format in a public s3 bucket. We move the data to our own location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for data_category in [\"train\", \"test\", \"validation\"]:\n",
    "    \n",
    "    data_key = \"{0}/{1}/abalone.{1}\".format(data_prefix, data_category)\n",
    "    output_key = \"{0}/{1}/abalone.{1}\".format(output_prefix, data_category)\n",
    "    data_filename = \"abalone.{}\".format(data_category)\n",
    "    s3_client.download_file(data_bucket, data_key, data_filename)   \n",
    "    s3_client.upload_file(data_filename, output_bucket, output_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d496d98",
   "metadata": {},
   "source": [
    "## Pulling and modifying the AWS XGBoost algorithm container \n",
    "\n",
    "The bash script `docker_build.sh` pulls the XGBoost image with the URI determined by the `image_uris.retrieve` method, modifies it, and pushes it to a private AWS ECR repository. (May need to run the script twice if authentication fails at first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5da58c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "version = '1.2-2'\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(region=region, framework='xgboost', version=version)\n",
    "# saving the image uri to extend later\n",
    "with open('src/base_image_uri.txt','w+') as f:\n",
    "    f.write(xgboost_container)\n",
    "custom_container=f\"{account}.dkr.ecr.{region}.amazonaws.com/custom_images/sagemaker-xgboost-{version}\"\n",
    "print(custom_container)\n",
    "!cd src && ./docker_build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f72f3",
   "metadata": {},
   "source": [
    "## Train a model to use in inference\n",
    "\n",
    "Next we train a model to use for inference. We have not modified the algorithm on the training side and could have used the original image for training as well. In any case, the training API is the same and we are using the builtin training script as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af01494",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"50\"}\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=custom_container, \n",
    "                                          base_job_name='xgboost-custom',\n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=f\"s3://{output_bucket}/{output_prefix}\")\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"libsvm\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/\".format(output_bucket, output_prefix, 'train'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/\".format(output_bucket, output_prefix, 'validation'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424cdf83",
   "metadata": {},
   "source": [
    "## Inference\n",
    "We use the batch transform mode to make predictions and compute SHAP values on the test data as sample. Inside the container, we have modified the data encoding so we can stack the vectorized inference outputs as a CSV-style output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=estimator.transformer(instance_count=1, instance_type= 'ml.m4.xlarge', \n",
    "                                  model_name='custom-xgboost', \n",
    "                                  output_path=f\"s3://{output_bucket}/{output_prefix}/results\")\n",
    "transformer.transform(f\"s3://{output_bucket}/{output_prefix}/test/abalone.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfbfd6b",
   "metadata": {},
   "source": [
    "### Inspecting the inference output\n",
    "We see the inference output now has the prediction, the expected apriori estimate (the expected shap value), and posteriori shap values that indicate feature contributions towards the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f787e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at output. Note that the third column value in original file output is always zero and corresponds to the label data in training phase \n",
    "df = pd.read_csv(f\"s3://{output_bucket}/{output_prefix}/results/abalone.test.out\", \n",
    "                 index_col=None, header=None, \n",
    "                 names=['prediction', 'base_values', 'shap 1', 'shap 2', 'shap 3', 'shap 4', 'shap 5', 'shap 6', 'shap 7', 'shap 8'], \n",
    "                 usecols=[0,1,3,4,5,6,7,8,9,10])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "shap.plots.force(df['base_values'][0], np.array(df[3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7683b1d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a guide on how to customize the production AWS XGBoost algorithm images to create a custom image that adheres to the base image as much as possible. Besides package consistency, a side benefit of this approach is that the training and inference APIs between the customized image and the builtin base image remain the same -- only the image URI needs to be swapped, which can be helpful to maintain consistency.  Plus, we can resuse many of the builtin functionalities that are already avaiable in the base image without code duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1865e1",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "The following cell cleans up the deployed resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3273fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"# Delete the CloudFormation stack.\\n\",\n",
    "\"# WARNING: THIS WILL DELETE THIS NOTEBOOK AND ANY CODE CHANGES.\\n\",\n",
    "\"# cft_client = boto3.client('cloudformation')\\n\",\n",
    "\"# cft_client.delete_stack(StackName='xgboost-extension-sample')\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
